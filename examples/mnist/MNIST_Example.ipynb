{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example guides a first-time user of QuantLib into the quantization process, using a small pretrained network and going through post-training per-layer quantization (i.e., representing weight and activation tensors as integers) and deployment (i.e., organizing operations so that they are an accurate representation of behavior in integer-based hardware).\n",
    "\n",
    "We will see how this operates through three stages: *FloatingPoint*, *FakeQuantized*, and *TrueQuantized*.\n",
    "QuantLib uses float32 tensors to represent data at all four stages - including *TrueQuantized*. \n",
    "This means that QuantLib code does not need special hardware support for integers to run on GPUs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by 1) performing the necessary imports, and 2) setting the target device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch version in use: 1.9.0+cu102 \n",
      "cuda avail:  True\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "#@title Imports & Set device\n",
    "\n",
    "#basic\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "#torch\n",
    "import torch; print('\\nPyTorch version in use:', torch.__version__, '\\ncuda avail: ', torch.cuda.is_available())\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "#quantlib!\n",
    "import quantlib\n",
    "import quantlib.algorithms as qa\n",
    "import quantlib.editing.graphs as qg\n",
    "import quantlib.editing.editing as qe\n",
    "import quantlib.backends.dory as qd\n",
    "\n",
    "device = 'cpu' # torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: %s' % device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first real step is to define the network topology. \n",
    "This works exactly like in a \"standard\" PyTorch script, using regular torch.nn.Module instances. \n",
    "QuantLib can transform most layers defined in torch.nn into its own representations. \n",
    "It will also perform a process of *Canonicalization* to make sure that common topological constructions,\n",
    "such as flattenization, residual layers terminating in an addition, and others are represented in a consistent fashion.\n",
    "\n",
    "As QuantLib exports `torch.fx` internally, we want all graphs related to identical functionality to be themselves identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define ExampleNet\n",
    "\n",
    "class ExampleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExampleNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 4, 3, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(4)\n",
    "        self.relu1 = nn.ReLU() # <== Module, not Function!\n",
    "\n",
    "        self.conv2 = nn.Conv2d(4, 20, 3, padding=1, stride=2, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(20)\n",
    "        self.relu2 = nn.ReLU() # <== Module, not Function!\n",
    "\n",
    "        self.conv3 = nn.Conv2d(20, 40, 3, padding=1, stride=2, bias=False)\n",
    "        self.bn3   = nn.BatchNorm2d(40)\n",
    "        self.relu3 = nn.ReLU() # <== Module, not Function!\n",
    "\n",
    "        self.conv4 = nn.Conv2d(40, 80, 3, padding=1, stride=2, bias=False)\n",
    "        self.bn4   = nn.BatchNorm2d(80)\n",
    "        self.relu4 = nn.ReLU() # <== Module, not Function!\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(80 * 4**2, 500, bias=False)\n",
    "        #self.fcbn = nn.BatchNorm1d(500)\n",
    "        self.fcrelu1 = nn.ReLU() # <== Module, not Function!\n",
    "        self.fc2 = nn.Linear(500, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))\n",
    "        x = self.relu3(self.bn3(self.conv3(x)))\n",
    "        x = self.relu4(self.bn4(self.conv4(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # x = self.flatten(x)\n",
    "        x = self.fcrelu1(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # output = F.log_softmax(x, dim=1) # <== the softmax operation does not need to be quantized, we can keep it as it is\n",
    "        return x\n",
    "        \n",
    "model = ExampleNet().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the testing functions (MNIST has no validation set).\n",
    "\n",
    "These are essentially identical to regular PyTorch code, with only one difference: testing (and validation) functions \n",
    "have a switch to support the production of non-negative integer data.\n",
    "\n",
    "This is important to test the last stage of quantization, i.e., *TrueQuantized*.\n",
    "\n",
    "Of course, this change might also be effectively performed inside the data loaders; \n",
    "in this example, we use standard `torchvision` data loaders for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fconti/miniconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "Validation: 100%|██████████| 78/78 [00:03<00:00, 19.74it/s, acc=0.99] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "FullPrecision accuracy: 0.990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Define Metrics and validation function\n",
    "\n",
    "# convenience class to keep track of averages\n",
    "class Metric(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.sum  = 0\n",
    "        self.n    = 0\n",
    "    def update(self, value):\n",
    "        self.sum += value\n",
    "        self.n += 1\n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.sum / self.n\n",
    "\n",
    "def validate(model, device, dataloader, verbose=True, integer=False):\n",
    "    model.eval()\n",
    "    loss    = 0\n",
    "    correct = 0\n",
    "    acc     = Metric('test_acc')\n",
    "    with tqdm(\n",
    "        total=len(dataloader), desc='Validation', disable=not verbose,\n",
    "        ) as t:\n",
    "        with torch.no_grad():\n",
    "            for data, target in dataloader:\n",
    "                if integer:      # support for production of\n",
    "                    data *= 255  # non-negative integer data\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "                pred = output.argmax(dim=1) # get index of largest log-probability\n",
    "                correct += pred.eq(target).sum().item()\n",
    "                acc.update(pred.eq(target).float().mean().item())\n",
    "                t.set_postfix({'acc': acc.avg})\n",
    "                t.update(1)\n",
    "    loss /= len(dataloader.dataset)\n",
    "    return acc.avg\n",
    "\n",
    "\n",
    "# calibration set\n",
    "Mcalib = 1024 # calibration set size\n",
    "train_set = datasets.MNIST('./data', train=True , download=True, transform=transforms.ToTensor())\n",
    "calib_set = torch.utils.data.Subset(train_set, indices=np.random.permutation(len(train_set))[:Mcalib])\n",
    "del train_set # we load a pretrained model, we won't train it in this script. Just needed for calibration!\n",
    "\n",
    "# validation set\n",
    "valid_set = datasets.MNIST('./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# set up the dataloaders\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "calib_loader = torch.utils.data.DataLoader(calib_set, batch_size=128, shuffle=False, drop_last=True, **kwargs)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=128, shuffle=False, drop_last=True, **kwargs)\n",
    "\n",
    "os.system('rm -rf examplenet.pt')\n",
    "os.system('wget https://github.com/MarcelloZanghieri2/NeMO_tutorial/blob/main/smallernet_4.pt?raw=true')\n",
    "os.system('mv smallernet_4.pt?raw=true examplenet.pt')\n",
    "\n",
    "model = ExampleNet().to(device)\n",
    "state_dict = torch.load('examplenet.pt', map_location='cpu')\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "acc = validate(model, device, valid_loader)\n",
    "print(\"\\n\\nFullPrecision accuracy: %.3f\" % (acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step toward quantization is tracing the graph of the model using the QuantLib tracer: the following cell performs this, then it prints a user-readable summary of the traced graph.\n",
    "To simplify the operation, we use the tracer embedded in `quantlib.graph.fx`, which wraps the `torch.fx` tracer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode       name     target    args               kwargs\n",
      "-----------  -------  --------  -----------------  --------\n",
      "placeholder  x        x         ()                 {}\n",
      "call_module  conv1    conv1     (x,)               {}\n",
      "call_module  bn1      bn1       (conv1,)           {}\n",
      "call_module  relu1    relu1     (bn1,)             {}\n",
      "call_module  conv2    conv2     (relu1,)           {}\n",
      "call_module  bn2      bn2       (conv2,)           {}\n",
      "call_module  relu2    relu2     (bn2,)             {}\n",
      "call_module  conv3    conv3     (relu2,)           {}\n",
      "call_module  bn3      bn3       (conv3,)           {}\n",
      "call_module  relu3    relu3     (bn3,)             {}\n",
      "call_module  conv4    conv4     (relu3,)           {}\n",
      "call_module  bn4      bn4       (conv4,)           {}\n",
      "call_module  relu4    relu4     (bn4,)             {}\n",
      "call_method  size     size      (relu4, 0)         {}\n",
      "call_method  view     view      (relu4, size, -1)  {}\n",
      "call_module  fc1      fc1       (view,)            {}\n",
      "call_module  fcrelu1  fcrelu1   (fc1,)             {}\n",
      "call_module  fc2      fc2       (fcrelu1,)         {}\n",
      "output       output   output    (fc2,)             {}\n"
     ]
    }
   ],
   "source": [
    "#@title Trace floating-point model graph and print it in human-readable format\n",
    "\n",
    "# Symbolic trace of the graph\n",
    "model_fp = qg.fx.quantlib_symbolic_trace(root=model)\n",
    "\n",
    "# Print the graph in tabular format\n",
    "model_fp.graph.print_tabular()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first \"real\" step for 8-bit quantization is the Float2Fake conversion, which is triggered by calling `quantlib.editing.float2fake.F2F8bitPACTRoundingConverter()`.\n",
    "This is a convenient editor that wraps inside a lot of useful transformations, such as:\n",
    " - canonicalization, i.e., transforming the network to a \"canonical\" format as previously discussed;\n",
    " - proper float2fake quantization, which replaces standard `nn.Conv2d`, `nn.Linear`, `nn.ReLU`, etc., into quantized modules - in  this case, `PACTConv2d`, `PACTLinear`, and `PACTReLU`, respectively;\n",
    " - folding of bias parameters into batch-normalization layers;\n",
    " - introduce quantization of non-activated tensors before Add layers;\n",
    " - set up rounding of weights and activations to avoid systematic bias in quantization; the latter is performed by folding the rounding bias into batch-normalization layers whenever possibile. This operation needs to be completed after calibration.\n",
    " \n",
    "`F2F8bitPACTRoundingConverter()` is a specialized version of `F2F8bitPACTConverter()`, and for special needs, it might be necessary to modify it by subclassing or adaptation. But for our purposes here, it is good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExampleNet(\n",
       "  (conv1): PACTConv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): PACTReLU()\n",
       "  (conv2): PACTConv2d(4, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): PACTReLU()\n",
       "  (conv3): PACTConv2d(20, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): PACTReLU()\n",
       "  (conv4): PACTConv2d(40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu4): PACTReLU()\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): PACTLinear(in_features=1280, out_features=500, bias=False)\n",
       "  (fcrelu1): PACTReLU()\n",
       "  (fc2): PACTLinear(in_features=500, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#float 2 fake\n",
    "f2fconverter = qe.float2fake.F2F8bitPACTRoundingConverter()\n",
    "model_fq = f2fconverter(model_fp)\n",
    "\n",
    "# set validation state\n",
    "model_fq.eval()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the `torch.fx` graph to see how the model has been subtly changed with respect to the Float version: notice in particular that the `flatten` node has been introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode       name     target    args        kwargs\n",
      "-----------  -------  --------  ----------  --------\n",
      "placeholder  x        x         ()          {}\n",
      "call_module  conv1    conv1     (x,)        {}\n",
      "call_module  bn1      bn1       (conv1,)    {}\n",
      "call_module  relu1    relu1     (bn1,)      {}\n",
      "call_module  conv2    conv2     (relu1,)    {}\n",
      "call_module  bn2      bn2       (conv2,)    {}\n",
      "call_module  relu2    relu2     (bn2,)      {}\n",
      "call_module  conv3    conv3     (relu2,)    {}\n",
      "call_module  bn3      bn3       (conv3,)    {}\n",
      "call_module  relu3    relu3     (bn3,)      {}\n",
      "call_module  conv4    conv4     (relu3,)    {}\n",
      "call_module  bn4      bn4       (conv4,)    {}\n",
      "call_module  relu4    relu4     (bn4,)      {}\n",
      "call_module  flatten  flatten   (relu4,)    {}\n",
      "call_module  fc1      fc1       (flatten,)  {}\n",
      "call_module  fcrelu1  fcrelu1   (fc1,)      {}\n",
      "call_module  fc2      fc2       (fcrelu1,)  {}\n",
      "output       output   output    (fc2,)      {}\n"
     ]
    }
   ],
   "source": [
    "# Print the graph in tabular format\n",
    "model_fq.graph.print_tabular()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases, we would see that the model at this stage is not fully functional due to lack of alignment between the quantization parameters (scale `eps` in particular) and the actual activations flowing through the network. For this MNIST experiment, it might even work! But in general, we need to\n",
    " 1. calibrate the network with real activation data\n",
    " 2. complete the rounding procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 8/8 [00:00<00:00, 24.07it/s, acc=0.997]\n",
      "Validation: 100%|██████████| 78/78 [00:02<00:00, 32.95it/s, acc=0.99] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FakeQuantized with calibration+rounding: accuracy: 0.990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# collect statistics about the floating-point `Tensor`s passing through the quantisers, so that we can better fit the quantisers' hyper-parameters\n",
    "with qe.float2fake.calibration(model_fq):\n",
    "    acc = validate(model_fq, device, calib_loader)\n",
    "\n",
    "# adds rounding to all PACT operators\n",
    "rounder =  qe.float2fake.F2F8bitPACTRounder()\n",
    "model_fq_rounded = rounder(model_fq)\n",
    "\n",
    "model_fq_rounded.to(device)\n",
    "model_fq_rounded.eval()\n",
    "\n",
    "acc = validate(model_fq_rounded, device, valid_loader)\n",
    "print(\"\\nFakeQuantized with calibration+rounding: accuracy: %.3f\" % (acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *FakeQuantized* network includes some quantization information (mainly for what concerns linear and activation layers), but it is not fully equivalent to a wholly quantized network.\n",
    "The Fake2True transformation completes the quantization of the network by making it so that all layers consume and produce integer Tensors, by means of a procedure of `eps` propagation -- i.e., propagation of scales throughout the network.\n",
    "To kickstart this process, QuantLib needs:\n",
    " 1. an example input (possibly with random values -- only the shape is necessary)\n",
    " 2. the scale to be attributed to input, i.e., the real value of a Tensor element represented with `1`\n",
    "\n",
    "The default converter is called `quantlib.editing.f2t.F2T24bitConverter()`. The number of bits mentioned refers to the number of bits allocated to intermediate accumulation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fconti/miniconda3/lib/python3.7/site-packages/torch/fx/graph.py:606: UserWarning: Attempted to insert a call_module Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule\n",
      "  warnings.warn(\"Attempted to insert a call_module Node with \"\n"
     ]
    }
   ],
   "source": [
    "# get exaple input\n",
    "x, _ = next(iter(valid_loader))\n",
    "x = x[0].unsqueeze(0)\n",
    "\n",
    "# convert to TrueQuantized with default 24-bit converter\n",
    "f2tconverter = qe.f2t.F2T24bitConverter()\n",
    "model_tq = f2tconverter(model_fq_rounded, {'x': {'shape': x.shape, 'scale': torch.tensor((0.0039216,))}})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print the *TrueQuantized* graph in readable format, we may see it looks different from before: all layers changed name, indicating their \"integerized\" nature; moreover, before the output you can stop a `EpsTunnel` layer that performs an important function: takes the integer output of the network and \"transforms\" it into a floating-point value, to ensure output consistency.\n",
    "The input of the *TrueQuantized* model, on the other hand, must already be integerized when put as input tot the network, hence we use `integer=True` when running validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/78 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode       name                                                   target                                             args                                                      kwargs\n",
      "-----------  -----------------------------------------------------  -------------------------------------------------  --------------------------------------------------------  --------\n",
      "placeholder  x                                                      x                                                  ()                                                        {}\n",
      "call_module  ql_eps_qconv2d_eps_integeriser_139755626488464__4_     QL_EpsQConv2dEpsIntegeriser_139755626488464__4_    (x,)                                                      {}\n",
      "call_module  ql_eps_bn2d_qre_lueps_requantiser_139755626052688__4_  QL_EpsBN2dQReLUEpsRequantiser_139755626052688__4_  (ql_eps_qconv2d_eps_integeriser_139755626488464__4_,)     {}\n",
      "call_module  ql_eps_qconv2d_eps_integeriser_139755626488464__3_     QL_EpsQConv2dEpsIntegeriser_139755626488464__3_    (ql_eps_bn2d_qre_lueps_requantiser_139755626052688__4_,)  {}\n",
      "call_module  ql_eps_bn2d_qre_lueps_requantiser_139755626052688__3_  QL_EpsBN2dQReLUEpsRequantiser_139755626052688__3_  (ql_eps_qconv2d_eps_integeriser_139755626488464__3_,)     {}\n",
      "call_module  ql_eps_qconv2d_eps_integeriser_139755626488464__2_     QL_EpsQConv2dEpsIntegeriser_139755626488464__2_    (ql_eps_bn2d_qre_lueps_requantiser_139755626052688__3_,)  {}\n",
      "call_module  ql_eps_bn2d_qre_lueps_requantiser_139755626052688__2_  QL_EpsBN2dQReLUEpsRequantiser_139755626052688__2_  (ql_eps_qconv2d_eps_integeriser_139755626488464__2_,)     {}\n",
      "call_module  ql_eps_qconv2d_eps_integeriser_139755626488464__1_     QL_EpsQConv2dEpsIntegeriser_139755626488464__1_    (ql_eps_bn2d_qre_lueps_requantiser_139755626052688__2_,)  {}\n",
      "call_module  ql_eps_bn2d_qre_lueps_requantiser_139755626052688__1_  QL_EpsBN2dQReLUEpsRequantiser_139755626052688__1_  (ql_eps_qconv2d_eps_integeriser_139755626488464__1_,)     {}\n",
      "call_module  flatten                                                flatten                                            (ql_eps_bn2d_qre_lueps_requantiser_139755626052688__1_,)  {}\n",
      "call_module  ql_eps_qlinear_eps_integeriser_139755626489168__2_     QL_EpsQLinearEpsIntegeriser_139755626489168__2_    (flatten,)                                                {}\n",
      "call_module  ql_eps_qre_lueps_requantiser_139755626052496__1_       QL_EpsQReLUEpsRequantiser_139755626052496__1_      (ql_eps_qlinear_eps_integeriser_139755626489168__2_,)     {}\n",
      "call_module  ql_eps_qlinear_eps_integeriser_139755626489168__1_     QL_EpsQLinearEpsIntegeriser_139755626489168__1_    (ql_eps_qre_lueps_requantiser_139755626052496__1_,)       {}\n",
      "call_module  ql_eps_tunnel_inserter_139755626446608__13_            QL_EpsTunnelInserter_139755626446608__13_          (ql_eps_qlinear_eps_integeriser_139755626489168__1_,)     {}\n",
      "output       output                                                 output                                             (ql_eps_tunnel_inserter_139755626446608__13_,)            {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 78/78 [00:01<00:00, 46.22it/s, acc=0.99] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TrueQuantized: accuracy: 0.990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the graph in tabular format\n",
    "model_tq.graph.print_tabular()\n",
    "\n",
    "# Test the network\n",
    "acc = validate(model_tq, device, valid_loader, integer=True)\n",
    "print(\"\\nTrueQuantized: accuracy: %.3f\" % (acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to export the network, e.g., to DORY, we need to remove the final `EpsTunnel`. QuantLib contains a suitable editor, which also returns in print format the integer-to-float scaling factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FinalEpsTunnelRemover] output: removing EpsTunnel with scaling factor tensor([[0.0003171186253894, 0.0002228131197626, 0.0001979214721359,\n",
      "         0.0002190211525885, 0.0001621314731892, 0.0001564017584315,\n",
      "         0.0002467445738148, 0.0001997630897677, 0.0001304127363255,\n",
      "         0.0002063974534394]])\n",
      "[FinalEpsTunnelRemover] output: outputs will need to be scaled *externally* to maintain program semantics.\n"
     ]
    }
   ],
   "source": [
    "epsremover = qe.f2t.FinalEpsTunnelRemover()\n",
    "model_tq_removed = epsremover(model_tq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final export of data to DORY for further testing can use the QuantLib embedded APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doryexporter = qd.DORYExporter()\n",
    "doryexporter.export(model_tq, x.shape, \".\")\n",
    "doryexporter.dump_features(model_tq, x, \".\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d9a4ac6aad1107c9d505078b8620968c21267201cf482e97e00220306d051b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
